{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Copy of tfidf-kmeans.ipynb","provenance":[{"file_id":"1geh5illX5RQ2jHIXwzsv-jKsbpoffQ80","timestamp":1639898325343}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"gRADssxtxiSa"},"source":["import re\n","import string\n","import pandas as pd\n","from functools import reduce\n","from math import log"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H8k3tzNwxiSf"},"source":["## Simple example of [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n","1. Example of corpus\n","2. Preprocessing and Tokenizing\n","3. Calculating bag of words\n","4. TF\n","5. IDF\n","6. TF-IDF"]},{"cell_type":"code","metadata":{"id":"UNxHkt-fxiSg"},"source":["\n","#1 simple-example-with\n","corpus = \"\"\"\n","Simple example with Cats and Mouse and Cats\n","Another simple example with dogs and cats\n","Another simple example with mouse and cheese\n","\"\"\".split(\"\\n\")[1:-1]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ep1397x0wuJz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605252217753,"user_tz":-420,"elapsed":1230,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"0365eec4-4aaf-4246-a17c-aa061b394075"},"source":["corpus"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Simple example with Cats and Mouse and Cats',\n"," 'Another simple example with dogs and cats',\n"," 'Another simple example with mouse and cheese']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"zwqT_fATxiSk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639897865012,"user_tz":-420,"elapsed":378,"user":{"displayName":"Khang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12576126383048471975"}},"outputId":"5c38884c-8e72-4570-bbd6-7f9e09040e42"},"source":["#2\n","l_A = corpus[0].lower().split()\n","l_B = corpus[1].lower().split()\n","l_C = corpus[2].lower().split()\n","\n","print(l_A)\n","print(l_B)\n","print(l_C)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['simple', 'example', 'with', 'cats', 'and', 'mouse', 'and', 'cats']\n","['another', 'simple', 'example', 'with', 'dogs', 'and', 'cats']\n","['another', 'simple', 'example', 'with', 'mouse', 'and', 'cheese']\n"]}]},{"cell_type":"code","metadata":{"id":"TOGa_70fxiSo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639897874842,"user_tz":-420,"elapsed":381,"user":{"displayName":"Khang Nguyen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12576126383048471975"}},"outputId":"627cfaf0-d89e-4d38-dd1a-de6e6bb420b1"},"source":["#3 vocabulary\n","word_set = set(l_A).union(set(l_B)).union(set(l_C))\n","print(word_set)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'simple', 'cheese', 'and', 'cats', 'example', 'dogs', 'another', 'mouse', 'with'}\n"]}]},{"cell_type":"code","metadata":{"id":"yUGh5BSoxiSs","colab":{"base_uri":"https://localhost:8080/","height":138},"executionInfo":{"status":"ok","timestamp":1605252227920,"user_tz":-420,"elapsed":1058,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"58f47aea-bea9-4a88-9be9-6bf074f521d3"},"source":["word_dict_A = dict.fromkeys(word_set, 0)\n","word_dict_B = dict.fromkeys(word_set, 0)\n","word_dict_C = dict.fromkeys(word_set, 0)\n","\n","for word in l_A:\n","    word_dict_A[word] += 1\n","\n","for word in l_B:\n","    word_dict_B[word] += 1\n","\n","for word in l_C:\n","    word_dict_C[word] += 1\n","\n","pd.DataFrame([word_dict_A, word_dict_B, word_dict_C])\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>dogs</th>\n","      <th>and</th>\n","      <th>with</th>\n","      <th>example</th>\n","      <th>another</th>\n","      <th>cheese</th>\n","      <th>mouse</th>\n","      <th>cats</th>\n","      <th>simple</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   dogs  and  with  example  another  cheese  mouse  cats  simple\n","0     0    2     1        1        0       0      1     2       1\n","1     1    1     1        1        1       0      0     1       1\n","2     0    1     1        1        1       1      1     0       1"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"vimQe1ySxiSv"},"source":["## \\#4 tf - term frequency\n","In the case of the term frequency $tf(t,d)$, the simplest choice is to use the raw count of a term in a string. \n","$${\\displaystyle \\mathrm {tf} (t,d)={\\frac {n_{t}}{\\sum _{k}n_{k}}}} $$\n","where $n_t$ is the number of occurrences of the word $t$ in the string, and in the denominator - the total number of words in this string."]},{"cell_type":"code","metadata":{"id":"uAkgsBzAxiSw"},"source":["def compute_tf(word_dict, l):\n","    tf = {}\n","    sum_nk = len(l)\n","    for word, count in word_dict.items():\n","        tf[word] = count/sum_nk\n","    return tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqRBV9qvxiSz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605252234384,"user_tz":-420,"elapsed":1123,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"bf08f8ea-5b6e-4962-9dd7-60a2646de968"},"source":["tf_A = compute_tf(word_dict_A, l_A)\n","tf_B = compute_tf(word_dict_B, l_B)\n","tf_C = compute_tf(word_dict_C, l_C)\n","tf_A"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'and': 0.25,\n"," 'another': 0.0,\n"," 'cats': 0.25,\n"," 'cheese': 0.0,\n"," 'dogs': 0.0,\n"," 'example': 0.125,\n"," 'mouse': 0.125,\n"," 'simple': 0.125,\n"," 'with': 0.125}"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"eEhiXhLHxiS2"},"source":["## \\#5 idf - inverse document frequency\n","idf is a measure of how much information the word provides\n","$$ \\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|} $$\n","- $N$: total number of strings in the corpus ${\\displaystyle N={|D|}}$\n","- ${\\displaystyle |\\{d\\in D:t\\in d\\}|}$  : number of strings where the term ${\\displaystyle t}$ appears (i.e., ${\\displaystyle \\mathrm {tf} (t,d)\\neq 0})$. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to ${\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}$."]},{"cell_type":"code","metadata":{"id":"zvHBXJsuxiS3"},"source":["def compute_idf(strings_list):\n","    n = len(strings_list)\n","    idf = dict.fromkeys(strings_list[0].keys(), 0)\n","    for l in strings_list:\n","        for word, count in l.items():\n","            if count > 0:\n","                idf[word] += 1\n","    \n","    for word, v in idf.items():\n","        idf[word] = log(n / float(v))\n","    return idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"706JLz2QxiS6"},"source":["idf = compute_idf([word_dict_A, word_dict_B, word_dict_C])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zjHwhVNIxiS9"},"source":["## \\# 6 tf-idf\n","Then tf–idf is calculated as\n","$$ {\\displaystyle \\mathrm {tfidf} (t,d,D)=\\mathrm {tf} (t,d)\\cdot \\mathrm {idf} (t,D)} $$"]},{"cell_type":"code","metadata":{"id":"iRC5pRLtxiS-"},"source":["def compute_tf_idf(tf, idf):\n","    tf_idf = dict.fromkeys(tf.keys(), 0)\n","    for word, v in tf.items():\n","        tf_idf[word] = v * idf[word]\n","    return tf_idf\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2p04Qd0RxiTB"},"source":["tf_idf_A = compute_tf_idf(tf_A, idf)\n","tf_idf_B = compute_tf_idf(tf_B, idf)\n","tf_idf_C = compute_tf_idf(tf_C, idf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMhW8gAxxiTE","colab":{"base_uri":"https://localhost:8080/","height":142},"executionInfo":{"status":"ok","timestamp":1569628391561,"user_tz":-420,"elapsed":1141,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"50cebe63-8c9b-4e89-8d09-235392f3da29"},"source":["pd.DataFrame([tf_idf_A, tf_idf_B, tf_idf_C])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>and</th>\n","      <th>another</th>\n","      <th>cats</th>\n","      <th>cheese</th>\n","      <th>dogs</th>\n","      <th>example</th>\n","      <th>mouse</th>\n","      <th>simple</th>\n","      <th>with</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.101366</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.050683</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>0.057924</td>\n","      <td>0.057924</td>\n","      <td>0.000000</td>\n","      <td>0.156945</td>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>0.057924</td>\n","      <td>0.000000</td>\n","      <td>0.156945</td>\n","      <td>0.000000</td>\n","      <td>0.0</td>\n","      <td>0.057924</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   and   another      cats    cheese      dogs  example     mouse  simple  with\n","0  0.0  0.000000  0.101366  0.000000  0.000000      0.0  0.050683     0.0   0.0\n","1  0.0  0.057924  0.057924  0.000000  0.156945      0.0  0.000000     0.0   0.0\n","2  0.0  0.057924  0.000000  0.156945  0.000000      0.0  0.057924     0.0   0.0"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"khkQWkJuxiTH"},"source":["# For clustering we must use tf-idf weights\n","the example above is just an example, in practice it is better to apply [TfidfVectorizer from sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"]},{"cell_type":"code","metadata":{"id":"z1O6XKHcxiTI"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s9qYbQfKxiTL"},"source":["## Full text for clusterring\n","\n","This corpus contain some strings about Google and some strings about TF-IDF from Wikipedia. Just for example"]},{"cell_type":"code","metadata":{"id":"XK-OrQd2xiTM"},"source":["all_text = \"\"\"\n","Google and Facebook are strangling the free press to death. Democracy is the loser\n","Your 60-second guide to security stuff Google touted today at Next '18\n","A Guide to Using Android Without Selling Your Soul to Google\n","Review: Lenovo’s Google Smart Display is pretty and intelligent\n","Google Maps user spots mysterious object submerged off the coast of Greece - and no-one knows what it is\n","Android is better than IOS\n","In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency\n","is a numerical statistic that is intended to reflect\n","how important a word is to a document in a collection or corpus.\n","It is often used as a weighting factor in searches of information retrieval\n","text mining, and user modeling. The tf-idf value increases proportionally\n","to the number of times a word appears in the document\n","and is offset by the frequency of the word in the corpus\n","\"\"\".split(\"\\n\")[1:-1]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bkgE3NnAxiTQ"},"source":["## Preprocessing and tokenizing\n","Firstly, we must bring every chars to lowercase and remove all punctuation, because it's not important for our task, but is very harmful for clustering algorithm. \n","After that, we'll split strings to array of words."]},{"cell_type":"code","metadata":{"id":"uGJDDb40xiTR"},"source":["def preprocessing(line):\n","    line = line.lower()\n","    line = re.sub(r\"[{}]\".format(string.punctuation), \" \", line)\n","    return line\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABNZATF9xiTU"},"source":["Now, let's calculate tf-idf for this corpus"]},{"cell_type":"code","metadata":{"id":"aOdBVquCxiTV","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1569628393345,"user_tz":-420,"elapsed":2912,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"0a9c8929-e525-4d41-d57e-88c0908b0f12"},"source":["tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocessing)\n","tfidf = tfidf_vectorizer.fit_transform(all_text)\n","tfidf.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(13, 93)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"nP155KVlxiTY"},"source":["And train simple kmeans model with k = 2"]},{"cell_type":"code","metadata":{"id":"Uoh3HSf9xiTZ"},"source":["kmeans = KMeans(n_clusters=2).fit(tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vIMto90exiTc"},"source":["Predictions"]},{"cell_type":"code","metadata":{"id":"RCABjzs2xiTd","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1569628393346,"user_tz":-420,"elapsed":2905,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"4d5ad76e-682f-490f-92ef-c688fbbbfa32"},"source":["lines_for_predicting = [\"Google and Facebook are strangling the free press to death. Democracy is the loser\", \"some androids is there\"]\n","tf_idf_data = tfidf_vectorizer.transform(lines_for_predicting)\n","kmeans.predict(tf_idf_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"U1Llg6xiJSAx","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1569628447132,"user_tz":-420,"elapsed":903,"user":{"displayName":"Huy Nguyễn Tiến","photoUrl":"","userId":"14788740065847059351"}},"outputId":"e2d250df-fce9-4ea8-a209-4ee9b046f443"},"source":["kmeans.transform(tfidf_vectorizer.transform(lines_for_predicting))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1.04677967, 0.88191068],\n","       [0.96925265, 1.0238864 ]])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"-mO754G0xiTk"},"source":[""],"execution_count":null,"outputs":[]}]}