{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "languageModeling-dl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCETV_3ojNIL",
        "outputId": "b344e52c-f3e8-454d-dfa4-0427acd948d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFkw9E3JET-Z",
        "outputId": "99310a32-81c6-4661-b976-108ead7b1212",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from keras import backend as K\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xly980DiQWC",
        "outputId": "a3209919-ce2a-48bd-d0c2-539312f51330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        }
      },
      "source": [
        "import pandas as pd\n",
        "#load data\n",
        "data_df = pd.read_csv(\"/content/drive/My Drive/SyncPC/Deep learning/iqfact_2019-11-19.csv\")\n",
        "data_df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>date</th>\n",
              "      <th>reactions</th>\n",
              "      <th>text</th>\n",
              "      <th>source_url</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Linh Ho√†ng</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>2019-11-19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>M·∫•y v·ª• chia s·∫ª du l·ªãch ch·ªâ th·∫•y ‚Äúc√°c c√¥ g√°i‚Äù c...</td>\n",
              "      <td>/lak.beiker?rc=p&amp;__tn__=R</td>\n",
              "      <td>https://mbasic.facebook.com/comment/replies/?c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IFact</td>\n",
              "      <td>Linh Ho√†ng</td>\n",
              "      <td>2019-11-19</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Ch·∫Øc con trai ƒëi t·ªën ti·ªÅn h∆°n \"P</td>\n",
              "      <td>/IQFact/?rc=p&amp;__tn__=R</td>\n",
              "      <td>https://mbasic.facebook.com/comment/replies/?c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>B·∫£o Nhii</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>2019-11-19</td>\n",
              "      <td>26.0</td>\n",
              "      <td>M·∫•y b·∫°n d·ªÖ th∆∞∆°ng m√¨nh chia s·∫ª t√≠. Ch·ªã &lt;e&gt;Nguy...</td>\n",
              "      <td>/profile.php?id=100040586285309&amp;rc=p&amp;__tn__=R</td>\n",
              "      <td>https://mbasic.facebook.com/comment/replies/?c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Thanh Y Lai</td>\n",
              "      <td>B·∫£o Nhii</td>\n",
              "      <td>2019-11-19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>üçÑ</td>\n",
              "      <td>/profile.php?id=100040417210432&amp;rc=p&amp;__tn__=R</td>\n",
              "      <td>https://mbasic.facebook.com/comment/replies/?c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>L√™ Duy Tr·ªçng</td>\n",
              "      <td>B·∫£o Nhii</td>\n",
              "      <td>2019-11-19</td>\n",
              "      <td>NaN</td>\n",
              "      <td>üå±</td>\n",
              "      <td>/profile.php?id=100040449008593&amp;rc=p&amp;__tn__=R</td>\n",
              "      <td>https://mbasic.facebook.com/comment/replies/?c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         source  ...                                                url\n",
              "0    Linh Ho√†ng  ...  https://mbasic.facebook.com/comment/replies/?c...\n",
              "1         IFact  ...  https://mbasic.facebook.com/comment/replies/?c...\n",
              "2      B·∫£o Nhii  ...  https://mbasic.facebook.com/comment/replies/?c...\n",
              "3   Thanh Y Lai  ...  https://mbasic.facebook.com/comment/replies/?c...\n",
              "4  L√™ Duy Tr·ªçng  ...  https://mbasic.facebook.com/comment/replies/?c...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX2gcHc_iQWH",
        "outputId": "1b79cabb-8f43-452f-b39e-f13dc285ed7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "def preprocessing(text):\n",
        "    text = re.sub(r\"\\<e\\>[^\\<]*\\<\\/e\\>\",\"@\",text)\n",
        "    text = text.lower()\n",
        "    return text+\"\\n\"\n",
        "     \n",
        "preprocessing(\"Co ha ,de d·∫´n m ƒëi\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'co ha ,de d·∫´n m ƒëi\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DhR5GiyiQWN"
      },
      "source": [
        "data_df = data_df.dropna(subset=['text'])\n",
        "data_df['text'] = data_df['text'].apply(lambda x: preprocessing(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQ9jSHTQEfJ9",
        "outputId": "b35bd14d-48a4-4b27-eefa-7335a09be716",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "\n",
        "char_tokenizer = Tokenizer(num_words=None, char_level=True)\n",
        "char_tokenizer.fit_on_texts(data_df['text'].values)\n",
        "\n",
        "#Only keep top 200 char\n",
        "filters = list(char_tokenizer.word_counts)[200:]\n",
        "\n",
        "seed_text_len = 15\n",
        "\n",
        "def create_seq(data):\n",
        "    length = seed_text_len\n",
        "    sequences = list()\n",
        "    for text in data:\n",
        "        #Filer text\n",
        "        result = list(filter(lambda x: x not in filters, text))\n",
        "        if (len(result) == 0):\n",
        "            continue\n",
        "        text = ''.join(result).strip()\n",
        "        for i in range(length, len(text)):\n",
        "            # select sequence of tokens\n",
        "            seq = text[i-length:i+1]\n",
        "            # store\n",
        "            sequences.append(seq)\n",
        "    print('Total Sequences: %d' % len(sequences))\n",
        "    return sequences\n",
        "\n",
        "# create sequences   \n",
        "sequences_raw = create_seq(data_df['text'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Sequences: 485618\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYBpbyjmiQWV",
        "outputId": "7000280d-3e3b-4828-9571-63ce079453cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sequences_raw[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['m·∫•y v·ª• chia s·∫ª d',\n",
              " '·∫•y v·ª• chia s·∫ª du',\n",
              " 'y v·ª• chia s·∫ª du ',\n",
              " ' v·ª• chia s·∫ª du l',\n",
              " 'v·ª• chia s·∫ª du l·ªã',\n",
              " '·ª• chia s·∫ª du l·ªãc',\n",
              " ' chia s·∫ª du l·ªãch',\n",
              " 'chia s·∫ª du l·ªãch ',\n",
              " 'hia s·∫ª du l·ªãch c',\n",
              " 'ia s·∫ª du l·ªãch ch',\n",
              " 'a s·∫ª du l·ªãch ch·ªâ',\n",
              " ' s·∫ª du l·ªãch ch·ªâ ',\n",
              " 's·∫ª du l·ªãch ch·ªâ t',\n",
              " '·∫ª du l·ªãch ch·ªâ th',\n",
              " ' du l·ªãch ch·ªâ th·∫•',\n",
              " 'du l·ªãch ch·ªâ th·∫•y',\n",
              " 'u l·ªãch ch·ªâ th·∫•y ',\n",
              " ' l·ªãch ch·ªâ th·∫•y ‚Äú',\n",
              " 'l·ªãch ch·ªâ th·∫•y ‚Äúc',\n",
              " '·ªãch ch·ªâ th·∫•y ‚Äúc√°',\n",
              " 'ch ch·ªâ th·∫•y ‚Äúc√°c',\n",
              " 'h ch·ªâ th·∫•y ‚Äúc√°c ',\n",
              " ' ch·ªâ th·∫•y ‚Äúc√°c c',\n",
              " 'ch·ªâ th·∫•y ‚Äúc√°c c√¥',\n",
              " 'h·ªâ th·∫•y ‚Äúc√°c c√¥ ',\n",
              " '·ªâ th·∫•y ‚Äúc√°c c√¥ g',\n",
              " ' th·∫•y ‚Äúc√°c c√¥ g√°',\n",
              " 'th·∫•y ‚Äúc√°c c√¥ g√°i',\n",
              " 'h·∫•y ‚Äúc√°c c√¥ g√°i‚Äù',\n",
              " '·∫•y ‚Äúc√°c c√¥ g√°i‚Äù ',\n",
              " 'y ‚Äúc√°c c√¥ g√°i‚Äù c',\n",
              " ' ‚Äúc√°c c√¥ g√°i‚Äù ch',\n",
              " '‚Äúc√°c c√¥ g√°i‚Äù chi',\n",
              " 'c√°c c√¥ g√°i‚Äù chia',\n",
              " '√°c c√¥ g√°i‚Äù chia ',\n",
              " 'c c√¥ g√°i‚Äù chia s',\n",
              " ' c√¥ g√°i‚Äù chia s·∫ª',\n",
              " 'c√¥ g√°i‚Äù chia s·∫ª ',\n",
              " '√¥ g√°i‚Äù chia s·∫ª c',\n",
              " ' g√°i‚Äù chia s·∫ª ch',\n",
              " 'g√°i‚Äù chia s·∫ª ch·ª©',\n",
              " '√°i‚Äù chia s·∫ª ch·ª© ',\n",
              " 'i‚Äù chia s·∫ª ch·ª© k',\n",
              " '‚Äù chia s·∫ª ch·ª© k ',\n",
              " ' chia s·∫ª ch·ª© k t',\n",
              " 'chia s·∫ª ch·ª© k th',\n",
              " 'hia s·∫ª ch·ª© k th·∫•',\n",
              " 'ia s·∫ª ch·ª© k th·∫•y',\n",
              " 'a s·∫ª ch·ª© k th·∫•y ',\n",
              " ' s·∫ª ch·ª© k th·∫•y c',\n",
              " 's·∫ª ch·ª© k th·∫•y co',\n",
              " '·∫ª ch·ª© k th·∫•y con',\n",
              " ' ch·ª© k th·∫•y con ',\n",
              " 'ch·ª© k th·∫•y con t',\n",
              " 'h·ª© k th·∫•y con tr',\n",
              " '·ª© k th·∫•y con tra',\n",
              " ' k th·∫•y con trai',\n",
              " 'k th·∫•y con trai ',\n",
              " ' th·∫•y con trai c',\n",
              " 'th·∫•y con trai ch',\n",
              " 'h·∫•y con trai chi',\n",
              " '·∫•y con trai chia',\n",
              " 'y con trai chia ',\n",
              " ' con trai chia s',\n",
              " 'con trai chia s·∫ª',\n",
              " 'on trai chia s·∫ª ',\n",
              " 'n trai chia s·∫ª n',\n",
              " ' trai chia s·∫ª nh',\n",
              " 'trai chia s·∫ª nh·ªù',\n",
              " 'rai chia s·∫ª nh·ªù ',\n",
              " 'ai chia s·∫ª nh·ªù =',\n",
              " 'i chia s·∫ª nh·ªù =)',\n",
              " ' chia s·∫ª nh·ªù =))',\n",
              " 'chia s·∫ª nh·ªù =)))',\n",
              " 'ch·∫Øc con trai ƒëi',\n",
              " 'h·∫Øc con trai ƒëi ',\n",
              " '·∫Øc con trai ƒëi t',\n",
              " 'c con trai ƒëi t·ªë',\n",
              " ' con trai ƒëi t·ªën',\n",
              " 'con trai ƒëi t·ªën ',\n",
              " 'on trai ƒëi t·ªën t',\n",
              " 'n trai ƒëi t·ªën ti',\n",
              " ' trai ƒëi t·ªën ti·ªÅ',\n",
              " 'trai ƒëi t·ªën ti·ªÅn',\n",
              " 'rai ƒëi t·ªën ti·ªÅn ',\n",
              " 'ai ƒëi t·ªën ti·ªÅn h',\n",
              " 'i ƒëi t·ªën ti·ªÅn h∆°',\n",
              " ' ƒëi t·ªën ti·ªÅn h∆°n',\n",
              " 'ƒëi t·ªën ti·ªÅn h∆°n ',\n",
              " 'i t·ªën ti·ªÅn h∆°n \"',\n",
              " ' t·ªën ti·ªÅn h∆°n \"p',\n",
              " 'm·∫•y b·∫°n d·ªÖ th∆∞∆°n',\n",
              " '·∫•y b·∫°n d·ªÖ th∆∞∆°ng',\n",
              " 'y b·∫°n d·ªÖ th∆∞∆°ng ',\n",
              " ' b·∫°n d·ªÖ th∆∞∆°ng m',\n",
              " 'b·∫°n d·ªÖ th∆∞∆°ng m√¨',\n",
              " '·∫°n d·ªÖ th∆∞∆°ng m√¨n',\n",
              " 'n d·ªÖ th∆∞∆°ng m√¨nh',\n",
              " ' d·ªÖ th∆∞∆°ng m√¨nh ',\n",
              " 'd·ªÖ th∆∞∆°ng m√¨nh c',\n",
              " '·ªÖ th∆∞∆°ng m√¨nh ch',\n",
              " ' th∆∞∆°ng m√¨nh chi',\n",
              " 'th∆∞∆°ng m√¨nh chia',\n",
              " 'h∆∞∆°ng m√¨nh chia ',\n",
              " '∆∞∆°ng m√¨nh chia s',\n",
              " '∆°ng m√¨nh chia s·∫ª',\n",
              " 'ng m√¨nh chia s·∫ª ',\n",
              " 'g m√¨nh chia s·∫ª t',\n",
              " ' m√¨nh chia s·∫ª t√≠',\n",
              " 'm√¨nh chia s·∫ª t√≠.',\n",
              " '√¨nh chia s·∫ª t√≠. ',\n",
              " 'nh chia s·∫ª t√≠. c',\n",
              " 'h chia s·∫ª t√≠. ch',\n",
              " ' chia s·∫ª t√≠. ch·ªã',\n",
              " 'chia s·∫ª t√≠. ch·ªã ',\n",
              " 'hia s·∫ª t√≠. ch·ªã @',\n",
              " 'ia s·∫ª t√≠. ch·ªã @ ',\n",
              " 'a s·∫ª t√≠. ch·ªã @ c',\n",
              " ' s·∫ª t√≠. ch·ªã @ c√≥',\n",
              " 's·∫ª t√≠. ch·ªã @ c√≥ ',\n",
              " '·∫ª t√≠. ch·ªã @ c√≥ k',\n",
              " ' t√≠. ch·ªã @ c√≥ k·∫π',\n",
              " 't√≠. ch·ªã @ c√≥ k·∫πo',\n",
              " '√≠. ch·ªã @ c√≥ k·∫πo ',\n",
              " '. ch·ªã @ c√≥ k·∫πo s',\n",
              " ' ch·ªã @ c√≥ k·∫πo so',\n",
              " 'ch·ªã @ c√≥ k·∫πo soc',\n",
              " 'h·ªã @ c√≥ k·∫πo soco',\n",
              " '·ªã @ c√≥ k·∫πo socol',\n",
              " ' @ c√≥ k·∫πo socola',\n",
              " '@ c√≥ k·∫πo socola ',\n",
              " ' c√≥ k·∫πo socola c',\n",
              " 'c√≥ k·∫πo socola c·ª±',\n",
              " '√≥ k·∫πo socola c·ª±c',\n",
              " ' k·∫πo socola c·ª±c ',\n",
              " 'k·∫πo socola c·ª±c n',\n",
              " '·∫πo socola c·ª±c ng',\n",
              " 'o socola c·ª±c ngo',\n",
              " ' socola c·ª±c ngon',\n",
              " 'socola c·ª±c ngon ',\n",
              " 'ocola c·ª±c ngon g',\n",
              " 'cola c·ª±c ngon gi',\n",
              " 'ola c·ª±c ngon gi√∫',\n",
              " 'la c·ª±c ngon gi√∫p',\n",
              " 'a c·ª±c ngon gi√∫p ',\n",
              " ' c·ª±c ngon gi√∫p g',\n",
              " 'c·ª±c ngon gi√∫p gi',\n",
              " '·ª±c ngon gi√∫p gi·∫£',\n",
              " 'c ngon gi√∫p gi·∫£m',\n",
              " ' ngon gi√∫p gi·∫£m ',\n",
              " 'ngon gi√∫p gi·∫£m m',\n",
              " 'gon gi√∫p gi·∫£m m·ª°',\n",
              " 'on gi√∫p gi·∫£m m·ª° ',\n",
              " 'n gi√∫p gi·∫£m m·ª° t',\n",
              " ' gi√∫p gi·∫£m m·ª° th',\n",
              " 'gi√∫p gi·∫£m m·ª° th·ª´',\n",
              " 'i√∫p gi·∫£m m·ª° th·ª´a',\n",
              " '√∫p gi·∫£m m·ª° th·ª´a ',\n",
              " 'p gi·∫£m m·ª° th·ª´a t',\n",
              " ' gi·∫£m m·ª° th·ª´a th',\n",
              " 'gi·∫£m m·ª° th·ª´a the',\n",
              " 'i·∫£m m·ª° th·ª´a theo',\n",
              " '·∫£m m·ª° th·ª´a theo ',\n",
              " 'm m·ª° th·ª´a theo n',\n",
              " ' m·ª° th·ª´a theo nh',\n",
              " 'm·ª° th·ª´a theo nhu',\n",
              " '·ª° th·ª´a theo nhu ',\n",
              " ' th·ª´a theo nhu c',\n",
              " 'th·ª´a theo nhu c·∫ß',\n",
              " 'h·ª´a theo nhu c·∫ßu',\n",
              " 't√†i nguy√™n c·ªßa c',\n",
              " '√†i nguy√™n c·ªßa c√¥',\n",
              " 'i nguy√™n c·ªßa c√¥ ',\n",
              " ' nguy√™n c·ªßa c√¥ g',\n",
              " 'nguy√™n c·ªßa c√¥ g√°',\n",
              " 'guy√™n c·ªßa c√¥ g√°i',\n",
              " 'uy√™n c·ªßa c√¥ g√°i ',\n",
              " 'y√™n c·ªßa c√¥ g√°i c',\n",
              " '√™n c·ªßa c√¥ g√°i c√≥',\n",
              " 'n c·ªßa c√¥ g√°i c√≥ ',\n",
              " ' c·ªßa c√¥ g√°i c√≥ s',\n",
              " 'c·ªßa c√¥ g√°i c√≥ s·∫µ',\n",
              " '·ªßa c√¥ g√°i c√≥ s·∫µn',\n",
              " 'a c√¥ g√°i c√≥ s·∫µn ',\n",
              " ' c√¥ g√°i c√≥ s·∫µn t',\n",
              " 'c√¥ g√°i c√≥ s·∫µn tr',\n",
              " '√¥ g√°i c√≥ s·∫µn tro',\n",
              " ' g√°i c√≥ s·∫µn tron',\n",
              " 'g√°i c√≥ s·∫µn trong',\n",
              " '√°i c√≥ s·∫µn trong ',\n",
              " 'i c√≥ s·∫µn trong n',\n",
              " ' c√≥ s·∫µn trong ng',\n",
              " 'c√≥ s·∫µn trong ng∆∞',\n",
              " '√≥ s·∫µn trong ng∆∞·ªù',\n",
              " ' s·∫µn trong ng∆∞·ªùi',\n",
              " 's·∫µn trong ng∆∞·ªùi ',\n",
              " '·∫µn trong ng∆∞·ªùi r',\n",
              " 'n trong ng∆∞·ªùi r ',\n",
              " ' trong ng∆∞·ªùi r c',\n",
              " 'trong ng∆∞·ªùi r c√≤']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwlD1taWiQWZ"
      },
      "source": [
        "# create a character mapping index\n",
        "chars = sorted(list(char_tokenizer.word_counts)[:200])\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "reverse_mapping = dict( (i,c) for i,c in enumerate(chars) )\n",
        "def encode_seq(seq):\n",
        "    sequences = list()\n",
        "    for line in seq:\n",
        "        # integer encode line\n",
        "        encoded_seq = [mapping[char] for char in line]\n",
        "        # store\n",
        "        sequences.append(encoded_seq)\n",
        "    return sequences\n",
        "\n",
        "# encode the sequences\n",
        "sequences = encode_seq(sequences_raw)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZkZpL7KINZF",
        "outputId": "7dc49fd4-43ad-4662-91f5-c53d61449bb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# vocabulary size\n",
        "vocab = len(mapping)\n",
        "sequences = np.array(sequences)\n",
        "# create X and y\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "# one hot encode y\n",
        "y = to_categorical(y, num_classes=vocab)\n",
        "# create train and validation sets\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (437056, 15) Val shape: (48562, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDAGndKAiQWg",
        "outputId": "fad93474-9946-43d4-bea4-144f93c55402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOWrAo3YiQWk"
      },
      "source": [
        "def model_cnn(seed_text_len, vocab):\n",
        "    nb_filter = 250\n",
        "    DENSE_HIDDEN_UNITS = 500\n",
        "    chars = Input(shape=(seed_text_len,))\n",
        "    print('Build model...')\n",
        "    x = Embedding(vocab, 40, input_length=seed_text_len, trainable=True)(chars)\n",
        "    def max_1d(X):\n",
        "        return K.max(X, axis=1)\n",
        "\n",
        "    # we add a Convolution1D, which will learn nb_filter\n",
        "    # word group filters of size 3:\n",
        "\n",
        "    con3_layer = Convolution1D(nb_filter=nb_filter,\n",
        "                        filter_length=2,\n",
        "                        border_mode='valid',\n",
        "                        activation='relu',\n",
        "                        subsample_length=1)(x)\n",
        "\n",
        "    pool_con3_layer = Lambda(max_1d, output_shape=(nb_filter,))(con3_layer)\n",
        "\n",
        "\n",
        "    # we add a Convolution1D, which will learn nb_filter\n",
        "    # word group filters of size 4:\n",
        "\n",
        "    con4_layer = Convolution1D(nb_filter=nb_filter,\n",
        "                        filter_length=3,\n",
        "                        border_mode='valid',\n",
        "                        activation='relu',\n",
        "                        subsample_length=1)(x)\n",
        "\n",
        "    pool_con4_layer =  Lambda(max_1d, output_shape=(nb_filter,))(con4_layer)\n",
        "\n",
        "\n",
        "    # we add a Convolution1D, which will learn nb_filter\n",
        "    # word group filters of size 5:\n",
        "\n",
        "    con5_layer = Convolution1D(nb_filter=nb_filter,\n",
        "                        filter_length=4,\n",
        "                        border_mode='valid',\n",
        "                        activation='relu',\n",
        "                        subsample_length=1)(x)\n",
        "\n",
        "    pool_con5_layer = Lambda(max_1d, output_shape=(nb_filter,))(con5_layer)\n",
        "\n",
        "\n",
        "\n",
        "    lstm_layer = GRU(300, recurrent_dropout=0.1, dropout=0.1,activation=\"sigmoid\")(x)\n",
        "\n",
        "    sent_emb_layer_model = concatenate([pool_con3_layer, pool_con5_layer,pool_con4_layer,lstm_layer])\n",
        "    cnn_layer = Dense(DENSE_HIDDEN_UNITS, activation='sigmoid')(sent_emb_layer_model)\n",
        "    x = Dropout(0.5)(cnn_layer)\n",
        "    result = Dense(vocab, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=chars, outputs=result)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3F2GN8zwiQWo",
        "outputId": "b243f754-6f5a-4da7-e85f-439c7386ff0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "check = ModelCheckpoint(\"cnn_lstm.hdf5\",monitor='val_acc',save_best_only=True,save_weights_only=True )\n",
        "\n",
        "\n",
        "# define model CNN\n",
        "cnn_model = model_cnn(seed_text_len, vocab)\n",
        "print(cnn_model.summary())\n",
        "\n",
        "# compile the model\n",
        "cnn_model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
        "# fit the model\n",
        "cnn_model.fit(X_tr, y_tr, epochs=50, verbose=1, validation_data=(X_val, y_val),batch_size=4048, callbacks=[check])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=250, kernel_size=2, strides=1, padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=250, kernel_size=3, strides=1, padding=\"valid\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=250, kernel_size=4, strides=1, padding=\"valid\")`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 15)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 15, 40)       8000        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 14, 250)      20250       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 12, 250)      40250       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 13, 250)      30250       embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 250)          0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 250)          0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 250)          0           conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     (None, 300)          306900      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 1050)         0           lambda_4[0][0]                   \n",
            "                                                                 lambda_6[0][0]                   \n",
            "                                                                 lambda_5[0][0]                   \n",
            "                                                                 gru_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 500)          525500      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 500)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 200)          100200      dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,031,350\n",
            "Trainable params: 1,031,350\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Train on 437056 samples, validate on 48562 samples\n",
            "Epoch 1/50\n",
            "437056/437056 [==============================] - 31s 70us/step - loss: 3.6729 - acc: 0.2134 - val_loss: 3.4074 - val_acc: 0.2410\n",
            "Epoch 2/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 3.1221 - acc: 0.2912 - val_loss: 2.8568 - val_acc: 0.3212\n",
            "Epoch 3/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.7762 - acc: 0.3301 - val_loss: 2.6004 - val_acc: 0.3478\n",
            "Epoch 4/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.5721 - acc: 0.3568 - val_loss: 2.4485 - val_acc: 0.3707\n",
            "Epoch 5/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.4527 - acc: 0.3749 - val_loss: 2.3581 - val_acc: 0.3878\n",
            "Epoch 6/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.3704 - acc: 0.3912 - val_loss: 2.2891 - val_acc: 0.4066\n",
            "Epoch 7/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.3042 - acc: 0.4048 - val_loss: 2.2277 - val_acc: 0.4202\n",
            "Epoch 8/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.2499 - acc: 0.4183 - val_loss: 2.1767 - val_acc: 0.4323\n",
            "Epoch 9/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.2039 - acc: 0.4269 - val_loss: 2.1351 - val_acc: 0.4390\n",
            "Epoch 10/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.1651 - acc: 0.4362 - val_loss: 2.1018 - val_acc: 0.4457\n",
            "Epoch 11/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.1321 - acc: 0.4427 - val_loss: 2.0748 - val_acc: 0.4511\n",
            "Epoch 12/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.1033 - acc: 0.4488 - val_loss: 2.0517 - val_acc: 0.4556\n",
            "Epoch 13/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.0781 - acc: 0.4542 - val_loss: 2.0330 - val_acc: 0.4584\n",
            "Epoch 14/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.0553 - acc: 0.4593 - val_loss: 2.0096 - val_acc: 0.4638\n",
            "Epoch 15/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.0340 - acc: 0.4635 - val_loss: 1.9929 - val_acc: 0.4683\n",
            "Epoch 16/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 2.0152 - acc: 0.4675 - val_loss: 1.9770 - val_acc: 0.4720\n",
            "Epoch 17/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9985 - acc: 0.4706 - val_loss: 1.9637 - val_acc: 0.4750\n",
            "Epoch 18/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9820 - acc: 0.4746 - val_loss: 1.9487 - val_acc: 0.4766\n",
            "Epoch 19/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9685 - acc: 0.4769 - val_loss: 1.9396 - val_acc: 0.4802\n",
            "Epoch 20/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9548 - acc: 0.4803 - val_loss: 1.9303 - val_acc: 0.4819\n",
            "Epoch 21/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9417 - acc: 0.4831 - val_loss: 1.9212 - val_acc: 0.4848\n",
            "Epoch 22/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9292 - acc: 0.4857 - val_loss: 1.9136 - val_acc: 0.4856\n",
            "Epoch 23/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9186 - acc: 0.4883 - val_loss: 1.9058 - val_acc: 0.4866\n",
            "Epoch 24/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.9073 - acc: 0.4909 - val_loss: 1.8970 - val_acc: 0.4903\n",
            "Epoch 25/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8965 - acc: 0.4931 - val_loss: 1.8886 - val_acc: 0.4914\n",
            "Epoch 26/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8865 - acc: 0.4953 - val_loss: 1.8845 - val_acc: 0.4927\n",
            "Epoch 27/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8774 - acc: 0.4969 - val_loss: 1.8815 - val_acc: 0.4947\n",
            "Epoch 28/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8675 - acc: 0.4995 - val_loss: 1.8725 - val_acc: 0.4948\n",
            "Epoch 29/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8589 - acc: 0.5009 - val_loss: 1.8680 - val_acc: 0.4963\n",
            "Epoch 30/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8507 - acc: 0.5033 - val_loss: 1.8613 - val_acc: 0.4978\n",
            "Epoch 31/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8423 - acc: 0.5043 - val_loss: 1.8569 - val_acc: 0.4987\n",
            "Epoch 32/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8350 - acc: 0.5065 - val_loss: 1.8535 - val_acc: 0.4998\n",
            "Epoch 33/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8255 - acc: 0.5084 - val_loss: 1.8504 - val_acc: 0.5011\n",
            "Epoch 34/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8187 - acc: 0.5097 - val_loss: 1.8468 - val_acc: 0.5023\n",
            "Epoch 35/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8108 - acc: 0.5118 - val_loss: 1.8424 - val_acc: 0.5030\n",
            "Epoch 36/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.8043 - acc: 0.5126 - val_loss: 1.8421 - val_acc: 0.5041\n",
            "Epoch 37/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7962 - acc: 0.5145 - val_loss: 1.8365 - val_acc: 0.5049\n",
            "Epoch 38/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7887 - acc: 0.5161 - val_loss: 1.8313 - val_acc: 0.5064\n",
            "Epoch 39/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7821 - acc: 0.5174 - val_loss: 1.8318 - val_acc: 0.5074\n",
            "Epoch 40/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7748 - acc: 0.5194 - val_loss: 1.8270 - val_acc: 0.5081\n",
            "Epoch 41/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7692 - acc: 0.5205 - val_loss: 1.8280 - val_acc: 0.5081\n",
            "Epoch 42/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7624 - acc: 0.5222 - val_loss: 1.8225 - val_acc: 0.5088\n",
            "Epoch 43/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7547 - acc: 0.5235 - val_loss: 1.8213 - val_acc: 0.5094\n",
            "Epoch 44/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7486 - acc: 0.5243 - val_loss: 1.8202 - val_acc: 0.5099\n",
            "Epoch 45/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7431 - acc: 0.5264 - val_loss: 1.8199 - val_acc: 0.5104\n",
            "Epoch 46/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7378 - acc: 0.5273 - val_loss: 1.8142 - val_acc: 0.5125\n",
            "Epoch 47/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7303 - acc: 0.5286 - val_loss: 1.8114 - val_acc: 0.5124\n",
            "Epoch 48/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7255 - acc: 0.5298 - val_loss: 1.8102 - val_acc: 0.5120\n",
            "Epoch 49/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7195 - acc: 0.5304 - val_loss: 1.8145 - val_acc: 0.5122\n",
            "Epoch 50/50\n",
            "437056/437056 [==============================] - 29s 67us/step - loss: 1.7134 - acc: 0.5315 - val_loss: 1.8082 - val_acc: 0.5133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcbaffd6128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT0WIBTbiQWt"
      },
      "source": [
        "import random\n",
        "import heapq\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, reverse_mapping, seq_length, seed_text, n_chars = 40):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # predict character\n",
        "        yhat = model.predict(encoded, verbose=0)[0]\n",
        "        #yhat = heapq.nlargest(5, range(len(yhat)), yhat.__getitem__)#top 5 indices        \n",
        "        yhat = np.argmax(yhat,axis=1)\n",
        "        #yhat = yhat[0]\n",
        "        \n",
        "        # reverse map integer to character\n",
        "        out_char = reverse_mapping[yhat]\n",
        "        \n",
        "        if out_char == \"\\n\":\n",
        "            break\n",
        "        # append to input\n",
        "        in_text += out_char\n",
        "    return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3XoVV7GJiQWx",
        "outputId": "446c12d0-8e6d-41a7-ab01-89a41588c88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "seed_text = \"mai m·ªët \"\n",
        "print(len(seed_text))\n",
        "generate_seq(cnn_model,mapping,reverse_mapping,seed_text_len,seed_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mai m·ªët nghi·ªáp nha :))))))))))))))))))))))))))))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiJbXlnXiQW0",
        "outputId": "7d277226-c942-43c9-9eab-8c09dad3836f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "import heapq\n",
        "\n",
        "a = np.array([[1,4,5],[1,7,9],[5,2,6]])\n",
        "y = [  heapq.nlargest(2, range(len(b)), b.__getitem__) for b in a]\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 1], [2, 1], [2, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}